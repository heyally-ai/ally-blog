<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Tools to connect to many LLMs at once - blog</title><meta name="description" content="To access the latest frontier AI models from Anthropic, OpenAI, Google, Perplexity, and Deepseek, you could pay $100 or more in monthly subscriptions. A cost-saving alternative is to use an AI chat interface — sometimes referred to as a “chatbot UI” or “LLM frontend” (there doesn’t appear to be an agreed upon naming convention). These apps allow you to connect to LLMs via their APIs. This way, you pay only for your actual usage. This approach&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><script async defer="defer" type="text/javascript" src="https://cloud.umami.is/script.js" data-website-id="d9f7c809-5bba-477d-8c96-401ce2b97db3" data-auto-track="true" data-do-not-track="true" data-cache="false"></script><link rel="canonical" href="https://heyally.ai/blog/tools-to-connect-to-many-llms-at-once/"><link rel="alternate" type="application/atom+xml" href="https://heyally.ai/blog/feed.xml" title="blog - RSS"><link rel="alternate" type="application/json" href="https://heyally.ai/blog/feed.json" title="blog - JSON"><meta property="og:title" content="Tools to connect to many LLMs at once"><meta property="og:site_name" content="blog"><meta property="og:description" content="To access the latest frontier AI models from Anthropic, OpenAI, Google, Perplexity, and Deepseek, you could pay $100 or more in monthly subscriptions. A cost-saving alternative is to use an AI chat interface — sometimes referred to as a “chatbot UI” or “LLM frontend” (there doesn’t appear to be an agreed upon naming convention). These apps allow you to connect to LLMs via their APIs. This way, you pay only for your actual usage. This approach&hellip;"><meta property="og:url" content="https://heyally.ai/blog/tools-to-connect-to-many-llms-at-once/"><meta property="og:type" content="article"><link rel="shortcut icon" href="https://heyally.ai/blog/media/website/favicon-64-x-64.ico" type="image/x-icon"><link rel="stylesheet" href="https://heyally.ai/blog/assets/css/style.css?v=fca5ead994f9661b5de4e88cfbf01439"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://heyally.ai/blog/tools-to-connect-to-many-llms-at-once/"},"headline":"Tools to connect to many LLMs at once","datePublished":"2025-12-07T14:21-05:00","dateModified":"2026-01-09T13:33-05:00","description":"To access the latest frontier AI models from Anthropic, OpenAI, Google, Perplexity, and Deepseek, you could pay $100 or more in monthly subscriptions. A cost-saving alternative is to use an AI chat interface — sometimes referred to as a “chatbot UI” or “LLM frontend” (there doesn’t appear to be an agreed upon naming convention). These apps allow you to connect to LLMs via their APIs. This way, you pay only for your actual usage. This approach&hellip;","author":{"@type":"Person","name":"Brian Gladu","url":"https://heyally.ai/blog/authors/brian-gladu/"},"publisher":{"@type":"Organization","name":"Brian Gladu"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="post-template"><header class="top js-header"><a class="logo" href="https://heyally.ai/blog/">blog</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="https://heyally.ai/blog/tags/updates/" target="_self">updates</a></li><li><a href="https://heyally.ai/blog/tags/productivity/" title="productivity" target="_self">productivity</a></li><li><a href="https://heyally.ai/blog/tags/ai/" title="ai" target="_self">ai</a></li></ul></nav></header><main class="post"><article class="content"><div class="hero hero--noimage"><header class="hero__content"><div class="wrapper"><h1>Tools to connect to many LLMs at once</h1></div></header></div><div class="entry-wrapper content__entry"><p>To access the latest frontier AI models from Anthropic, OpenAI, Google, Perplexity, and Deepseek, you could pay $100 or more in monthly subscriptions.</p><p>A cost-saving alternative is to use an AI chat interface — sometimes referred to as a “chatbot UI” or “LLM frontend” (there doesn’t appear to be an agreed upon naming convention). These apps allow you to connect to LLMs via their APIs. This way, you pay only for your actual usage.</p><p>This approach has a few advantages:</p><ol><li><p><strong>Multi-model access</strong> - With a frontend UI you can access any model with an OpenAPI compatible API. You could be coding with Claude, then switch to Deepseek to plan your next vacation, then throw an enormous PDF at Gemini for some quick Q&amp;A.</p></li><li><p><strong>Cost-savings</strong> - What’s great about accessing LLMs via the API is that, for most people, it is much less expensive because you’re only paying for your actual usage. Unless you’re an extremely heavy user, you’ll probably find that your total API costs — across all models — ends up being less than $10 a month.</p></li><li><p><strong>Cool features</strong> - Chatbot UI’s tend to clone features added by the major players and will often explore experimental or highly speclialized new features you won’t find in any of the UIs of the major AI players. You’ll be able to run open source models locally, test multiple models side-by-side, easily add RAG to give your AI specialized knowledge, and more.</p></li><li><p><strong>Easier to manage</strong> - It’s much easier to manage prompts, system instructions, stored memories, and RAG databases from one place instead of needing to keep this kind of LLM context data consistent and up-to-date across the various LLM platforms.</p></li></ol><p>And this approach has downsides:</p><ol><li><p><strong>You’ll lose access to some of the specialized, unique features</strong> - The Perplexity UI has some great features like Projects and Pages. OpenAI’s desktop app can connect to apps you’re running locally so it can “see” what you’re doing. Sometimes these features have been re-created by the frontend UIs in this list and sometimes they haven’t. More than once since cancelling my subscription to the big providers, I’ve signed back up for a month or two to test out their latest features. Usually I find that whatever feature it was isn’t as valuable as I’d hoped and I return to using my frontend interface.</p></li><li><p><strong>You’ll need to invest time up front</strong> - There’s a little more work involved with getting this set up, but don’t let it intimidate you. You’ll need to sign up for API keys for the models you want to use, which is tedious since you’ll need to input your billing info for each one. If you want to add all the major models you’ll probably be looking at 30 minutes or so. But, you can dip your toe into this without commiting to connecting to 10 different LLM APIs. You can connect just one API, use the app for a bit, and decide whether you want to test a different one or add more more models.</p></li><li><p><strong>Less polish</strong> - I love the OpenAI UI and haven’t found an alternative chatbot UI that’s as visually pleasing, responsive, and organized. It’s not perfect, but I find myself wishing that one of these apps would just clone it and add folders and workspaces. Also, most of these chatbot UI apps are undergoing active development from small teams so they change quickly, things break, etc. These apps aren’t going through the same extensive product testing and validation processes Google, OpenAI, etc. are using before they release an update.</p></li><li><p><strong>Time spent tinkering</strong> - This is really an add on to item 3. Due to “lack of polish,” these apps tend to be more buggy and have more issues than you’re probably used to. Depending on whether you’re interested in learning new tech skills, you’ll find the time you spend setting this up and using it day to day to be a minor annoyance that doubles as a learning opportunity, or an intolerable inconvenience and waste of precious time. If you simply “need things to work” more than you’re interested in learning — about LLMs and tech in general — this may not be the right path for you.</p></li></ol><h2 id="about-this-list-of-llm-interfaces">About this list of LLM interfaces</h2><p>Before we jump into the list, let’s talk a bit about methodology, perspective, and biases. I’ve tried to pull out the pros and cons that seem important to someone looking for an LLM interface but, for instance, I’m not a coder, so I don’t have insight into which UI is best for software development and I don’t bother to review Cursor and other UIs specializing in code generation.</p><p>I take into account:</p><ul><li><strong>Cost / licensing</strong> - Most of the apps are free but I point out where some features are gated behind a paid plan.</li><li><strong>Tech savviness required</strong> - For instance, do you need to use or understand code?</li><li><strong>Installation process</strong> - Does it offer Docker installation or will you be installing dependencies via command line?</li><li><strong>Web-based or native app?</strong> Can it be run locally? Can it be self-hosted and accessed online?</li><li><strong>Ability to run local and API models?</strong> Does it integrate with ollama or similar?</li><li><strong>Mobile experience</strong> - Is it well-optimized for small screens?</li><li><strong>Audio experience</strong> - How does the TTS and STT experience measure up? I use OpenAI’s advanced speech mode as my comparison.</li><li><strong>My overall rating</strong> - This is where I attempt to rate each of these apps based on my overall experience with it. It’s not an average of all the other scores, or a ranking by the total of the scores, or anything like that. It’s just my opinion.</li></ul><p>I have no personal connection to the developers of these apps and I am not using any affiliate links or benefiting financially in any way.</p><p>If you have an app you’d like to see listed here, I’d love to try it, email me at: <a href="mailto:&#x62;&#114;&#x69;&#97;&#110;&#103;&#x6c;&#x61;&#x64;&#117;&#x40;&#103;&#109;&#x61;&#x69;&#108;&#x2e;&#x63;&#111;&#109;">&#x62;&#114;&#x69;&#97;&#110;&#103;&#x6c;&#x61;&#x64;&#117;&#x40;&#103;&#109;&#x61;&#x69;&#108;&#x2e;&#x63;&#111;&#109;</a>. If you’re the developer of one of these apps and you’d like to discuss something I’ve said here, I’d love to chat! Don’t hesitate to email me.</p><h1 id="browser-based-solutions">Browser-based solutions</h1><p>These apps can be exposed to the web or they can be locally accessed (via a browser). They often install via Docker. If you aren’t already using Docker apps, I highly recommend you dip your toe into it! It opens up a world of open source software and it really isn’t as technically challenging as it may appear. In fact, the whole point of it is to make setting up and using apps easier. That’s a topic for a different post though.</p><h2 id="openwebui"><a href="https://github.com/open-webui/open-webui">openwebui</a></h2><p><a href="https://github.com/open-webui/open-webui">Github stars: 70k</a> | <a href="https://docs.openwebui.com/roadmap/">Roadmap</a></p><blockquote><p>TL;DR: Openwebui’s standout feature is its community of user-created functions (that are used to add custom functionality). It’s popular because it’s one of the best chatbot UI’s available at the moment.</p></blockquote><p>One of the better-known AI interfaces, openwebui is also one of the most powerful and full-featured. It’s extensive feature set includes memory management, support for TTS and STT (audio), workspaces (which allow you to specify custom instructions), tags and folders for conversation management, and more.</p><p>It’s a web app that can be run locally (you’ll still access it via a browser window). It can be set up via Docker, which can make the installation process much faster and less error-prone for the less technical.</p><p>OpenwebUI’s major, standout strength is its community and extensibility. A <a href="https://openwebui.com/">web-based community portal</a> offers a collection of community-developed custom functions that add functionality and capabilities. It also offers a companion app, <a href="https://github.com/open-webui/pipelines">Pipelines</a>, for adding major functionality like supporting new model providers. I imagine for a developer, the openness and extensibility of openwebui is a lot of fun. For me, a non-developer, it can sometimes be challenging. Functions or Pipelines I install from the community don’t always work or are buggy. This can give the app an unpolished or unstable feel that I find unpleasant since I use AI constantly throughout the day at work and at home. If you need it to “just work” you might find a vanilla installation works great but some of the community functions cause issues.</p><p>The UI itself is good, but not great. The mobile UI especially could use some optimization. Folders were added recently which is a huge step up from the tags-only organization system it had before. Visually, the folder icons are quite small which is an odd design choice I’m seeing more and more. Also, in other apps you’re able to add custom instructions onto folders, but openwebui does not currently offer that.</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Price</td><td>Free</td></tr><tr><td>Open source?</td><td>Yes</td></tr><tr><td>Technical skills required</td><td>Docker</td></tr><tr><td>Installation</td><td>Docker</td></tr><tr><td>Browser-based or native?</td><td>Browser</td></tr><tr><td>Run local models?</td><td>Yes</td></tr><tr><td>Mobile experience</td><td>3/5</td></tr><tr><td>Audio experience</td><td>3/5</td></tr><tr><td>Overall rating</td><td>4/5</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="standout-features">Standout features:</h3><ul><li>A large community of many user-created custom functions (plugins) is openwebui’s “killer feature” you won’t find anywhere else.</li><li>Create multiple workspaces for separation and shared context between conversations</li><li>Speech support. Many frontend UI’s for AI don’t offer this. It’s not up to OpenAI’s Advanced Voice mode, yet. There’s a signficant delay between what you say and the LLM’s response. Also, no interruption (or even listening) while the LLMs response is being read.</li><li>Memory support that works similarly to OpenAI’s though out of the box you’ll need to click an “add to memory” button. There are also “autolearning” functions available via the community but I haven’t gotten these to work reliably yet.</li></ul><h3 id="weaknesses">Weaknesses:</h3><ul><li>Conversation organization</li><li>Speech mode is far behind what the frontier models are offering</li><li>Community functions aren’t well-curated - can be buggy and unreliable</li></ul><h2 id="chatbotui">chatbotui</h2><p><a href="https://github.com/mckaywrigley/chatbot-ui">29.6k github stars</a> | Not maintained</p><p>This entry will be short and it pains me that it is. What began as one of the most promising interfaces seems to have become an abandoned project. It’s still second only to the OpenAI UI in my mind. You can still set it up and use it, it just hasn’t kept pace with the rapid developments in the space. If you just want to have conversations with a chatbot and organize your threads into folders, this might be a great app for you. Just be aware that no one is patching security holes or adding new features at this time. One of the unfortunate downsides to the open source software scene is that isn’t uncommon.</p><p>If you find an app you like, look for an opportunity to donate to the developers. It really, really does make a difference.</p><h1 id="options-that-run-locally">Options that run locally</h1><p>These apps don’t require Docker or touching a command line tool so they’re a great option for anyone who just wants to get up and running with an API frontend without learning something new.</p><p>I do encourage you to learn Docker and get experience using command line tools. Neither is as intimidating or difficult as they may seem. You’ll need patience more than technical skills or intelligence.</p><h2 id="msty"><a href="https://msty.app/">msty</a></h2><p>Not on github &amp; not open source Free, with some perk-like features accessible with a one-time payment</p><p>I find myself using both Msty and OpenWebUI a lot. They offer similar features and they’re both powerful — you can tweak as many model parameters as you like. They both make it really easy to use a combinaton of local models (via ollama) and proprietary models (via APIs). It’s really cool comparing the output from some of the smaller open source models to the output of the bleeding edge models.</p><p>Msty stands out in my mind as having a better folder and conversation organization system than openwebui (you can add custom instructions onto folders so that all conversations created in that folder are given context for that topic/project), it has better model comparison tools (you can send the same prompt to 2 or more models at once, split off conversations, and more.</p><p>While I’m comparing it to openwebui, I should point out one of openwebui’s strengths – its openness. There’s a large community of openwebui who contribute functions which are basically customizations or enhancements or even new features. It can be hit or miss on how buggy they are, but overall it’s a nice, signficant advantage openwebui has over msty, as a closed source product, will likely never have.</p><p>There are also some cool features you won’t see anywhere else like the ability to dig deeper into key words. It’s free, but there is a paid option that gives you early access to new features and unlocks some bells and whistles. Some companies aggressively gate their best features behind paid plans but Msty gives you even its most powerful features free. You’d get the paid plan mostly to support its development vs upgrading because you need access to this or that feature to get your work done. I really like that approach.</p><p>I do wish Msty was snappier. For an app running natively and locally, I expect it to be extremely responsive but it isn’t, at times. For instance, when switching between workplaces. Even smaller operations like switching between conversations feels just a titch laggy.</p><p>Another criticism would be that I think the UI could use some work. I’m not sure if it’s a new design trend or what, but a lot of LLM frontend apps use very small icons for the folders. This is unintuitive to me because in my mind folders should be bigger (since, they’re collections of all the files within them). It just goes against what I expect for folders to be visually smaller than files. It’s a small thing, but it bugs me. Another small nuisance that plagues me is that, even though it can be customized and tweaked in some ways, I can’t get the typography to feel quite right. There are other UI elements that could use some tweaking and polish in my mind. None of them is a dealbreaker and it’s unfair of me to mention so many slight imperfections for Msty and gloss over some much more significant ones in other apps. Just take into account that I use it a lot so it’s imperfections bug me more often and are more top of mind.</p><p>It’s a great app and the fact that I use it more than any other says a lot and reveals my opinion better than my focus on its minor flaws.</p><h2 id="jan"><a href="https://jan.ai/">jan</a></h2><p>Jan is an interesting app because if you fit its use case it’s perfect, but more advanced users will find it unuseable.</p><p>It’s available for Mac, Windows, and Linux. It makes running local models really easy. There’s a tab where you can quickly browse and download new models. It even warns you if a model is likely to struggle running on your system. In settings, you’re able to add API keys for a solid list of hosted providers. If that’s all you’re looking for, this is probably a really solid choice.</p><p>However, if you’re looking for pretty much any power feature — like folders, memory management, side-by-side conversations, or even access to advanced model parameter settings — this probably isn’t the app for you.</p><p>It feels snappy and polished. It looks like new features are well-planned and well-execuated. It’s one of those apps that feels like it was created by a thoughtful, passionate developer.</p><p>But without more advanced features and functionality, I think it will rarely be at the top of anyone’s list. The features it lacks are kind of essential for anyone who is using LLMs professionally, interested in prompt engineering, etc.</p><h2 id="lm-studio">LM studio</h2><p>Known to have the best performance because it’s utilizing a faster underlying technology (MLX) than llama.cpp and other solutions for local inference.</p><h2 id="ally"><a href="https://heyally.ai">Ally</a></h2><p>A new entry into the local LLM inference space, Ally AI gives Mac users access to the latest and greatest models like Qwen3, Gemma3, GPT-OSS, and more.</p><p>Ally’s standout feature is its “Shared Context” which makes it so that you can collaborate with AI via a shared Markdown document.</p><h2 id="ollama">Ollama</h2></div><footer class="content__footer"><div class="entry-wrapper"><p class="content__updated">This article was updated on January 9, 2026</p><div class="content__actions"><ul class="content__tag"><li><a href="https://heyally.ai/blog/tags/ai/">ai</a></li><li><a href="https://heyally.ai/blog/tags/llms/">llms</a></li></ul></div></div></footer></article><div class="content__related related"><div class="wrapper"><h2 class="h4 related__title">You should also read:</h2><article class="feed__item feed__item--centered"><div class="feed__content"><header><h3 class="feed__title"><a href="https://heyally.ai/blog/the-state-of-running-llms-locally-in-2026/">Running LLMs locally in 2025-26: what to expect + how to get started</a></h3></header><p>If you’re excited to run open source LLMs like GPT-OSS, Qwen3, and Gemma on your laptop, desktop, or phone as a learning experience, or just for the technerdy coolness of running actual AI locally, on your own device, you’ll have a great time as long as your expectations are aligned with how AI runs on consumer hardware. But, if you’re hoping it will be like having your own Claude 4.5 or ChatGPT 4.2 running locally,&hellip;</p><a href="https://heyally.ai/blog/the-state-of-running-llms-locally-in-2026/" class="readmore feed__readmore">Continue reading...</a></div></article></div></div></main><footer class="footer footer--glued"><div class="wrapper"><div class="footer__copyright"><p><a href="https://heyally.ai">Back to Ally AI main site</a></p></div><button id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg width="20" height="20"><use xlink:href="https://heyally.ai/blog/assets/svg/svg-map.svg#toparrow"/></svg></button></div></footer><script defer="defer" src="https://heyally.ai/blog/assets/js/scripts.min.js?v=ffcbea6c02c8178d10092962b235a5b0"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script><script src="https://heyally.ai/blog/media/plugins/pagePrefetching/quicklink.umd.js"></script><script>window.addEventListener('load', () => {
					quicklink.listen({
						prerender: true,
						el: document.querySelector('body'),
						delay: 0,
						limit: Infinity,
						throttle: Infinity,
						timeout: 2000
					});
				});</script></body></html>