<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Running LLMs locally in 2025-26: what to expect + how to get started - blog</title><meta name="description" content="If you’re excited to run open source LLMs like GPT-OSS, Qwen3, and Gemma on your laptop, desktop, or phone as a learning experience, or just for the technerdy coolness of running actual AI locally, on your own device, you’ll have a great time as long as your expectations are aligned with how AI runs on consumer hardware. But, if you’re hoping it will be like having your own Claude 4.5 or ChatGPT 4.2 running locally,&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><script async defer="defer" type="text/javascript" src="https://cloud.umami.is/script.js" data-website-id="d9f7c809-5bba-477d-8c96-401ce2b97db3" data-auto-track="true" data-do-not-track="true" data-cache="false"></script><link rel="canonical" href="https://heyally.ai/blog/the-state-of-running-llms-locally-in-2026/"><link rel="alternate" type="application/atom+xml" href="https://heyally.ai/blog/feed.xml" title="blog - RSS"><link rel="alternate" type="application/json" href="https://heyally.ai/blog/feed.json" title="blog - JSON"><meta property="og:title" content="Running LLMs locally in 2025-26: what to expect + how to get started"><meta property="og:site_name" content="blog"><meta property="og:description" content="If you’re excited to run open source LLMs like GPT-OSS, Qwen3, and Gemma on your laptop, desktop, or phone as a learning experience, or just for the technerdy coolness of running actual AI locally, on your own device, you’ll have a great time as long as your expectations are aligned with how AI runs on consumer hardware. But, if you’re hoping it will be like having your own Claude 4.5 or ChatGPT 4.2 running locally,&hellip;"><meta property="og:url" content="https://heyally.ai/blog/the-state-of-running-llms-locally-in-2026/"><meta property="og:type" content="article"><link rel="shortcut icon" href="https://heyally.ai/blog/media/website/favicon-64-x-64.ico" type="image/x-icon"><link rel="stylesheet" href="https://heyally.ai/blog/assets/css/style.css?v=fca5ead994f9661b5de4e88cfbf01439"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://heyally.ai/blog/the-state-of-running-llms-locally-in-2026/"},"headline":"Running LLMs locally in 2025-26: what to expect + how to get started","datePublished":"2025-12-07T15:40-05:00","dateModified":"2026-01-10T13:39-05:00","description":"If you’re excited to run open source LLMs like GPT-OSS, Qwen3, and Gemma on your laptop, desktop, or phone as a learning experience, or just for the technerdy coolness of running actual AI locally, on your own device, you’ll have a great time as long as your expectations are aligned with how AI runs on consumer hardware. But, if you’re hoping it will be like having your own Claude 4.5 or ChatGPT 4.2 running locally,&hellip;","author":{"@type":"Person","name":"Brian Gladu","url":"https://heyally.ai/blog/authors/brian-gladu/"},"publisher":{"@type":"Organization","name":"Brian Gladu"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript><style>.extlink::after {
				background-color: currentColor;
				content: "";
				display: inline-block;
				height: 16px;
				margin-left: 5px;
				position: relative;
				top: 0px;
				width: 16px;
			}
		
					.extlink.extlink-icon-1::after {
						mask-image: url("data:image/svg+xml;utf8,<svg viewBox='0 0 24 24' fill='none' stroke='%23000000' stroke-width='2' stroke-linecap='round' stroke-linejoin='round' xmlns='http://www.w3.org/2000/svg'><path d='M15 3h6v6'/><path d='M10 14 21 3'/><path d='M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6'/></svg>");
						mask-repeat: no-repeat;
						mask-size: contain;
					}</style></head><body class="post-template"><header class="top js-header"><a class="logo" href="https://heyally.ai/blog/">blog</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="https://heyally.ai/blog/tags/updates/" target="_self">updates</a></li><li><a href="https://heyally.ai/blog/tags/productivity/" title="productivity" target="_self">productivity</a></li><li><a href="https://heyally.ai/blog/tags/ai/" title="ai" target="_self">ai</a></li></ul></nav></header><main class="post"><article class="content"><div class="hero hero--noimage"><header class="hero__content"><div class="wrapper"><h1>Running LLMs locally in 2025-26: what to expect + how to get started</h1></div></header></div><div class="entry-wrapper content__entry"><p>If you’re excited to run open source LLMs like GPT-OSS, Qwen3, and Gemma on your laptop, desktop, or phone as a learning experience, or just for the technerdy coolness of running actual AI locally, on your own device, you’ll have a great time <em>as long as your expectations are aligned with how AI runs on consumer hardware</em>.</p><p>But, if you’re hoping it will be like having your own Claude 4.5 or ChatGPT 4.2 running locally, you’ll be disappointed for three reasons:</p><ol><li><p><strong>The open source models that can be run on consumer hardware are not comparable to frontier models</strong> created by Google, OpenAI, Anthropic, and the other “big players.” Using Qwen3 8b with 4-bit quantization isn’t going to compare to using GPT-5.2 Thinking. Even running GPT-OSS 120b, if your device can handle it, isn’t going to be comparable. These smaller models that can run on consumer hardware, even the latest (expensive) desktop hardware, simply are not as intelligent as the trillion+ parameter frontier models.</p></li><li><p><strong>What you’ll run locally will be a more raw user experience.</strong> It won’t be as refined or polished because it won’t be wrapped in the unseen backend layers that enhance the user experience of commercial chat products like ChatGPT, Gemini, and Claude. Locally, you’ll hit an LLM with a chat request and get output back. There’s no sophisticated RAG that gives the model context from your other chats, no memory solution for retaining key facts, no multi-step agentic processes for things like deep research. These often transparent enhancements simply aren’t there augmenting and improving the user experience.</p></li><li><p><strong>It’s going to be a lot slower.</strong> Even on an M-series Mac utilizing MLX with ample RAM, you’re not going to get the consistent near-instant time to first token and 100+ tokens per second firehouse of streamed output. It can feel like very slow after becoming acclimated to the stellar performance of the big chatbot solutions.</p></li></ol><p>So, you’re going to be using a less intelligent model, it’s not going to be as capable, and it’s going to be slower.</p><p>But <em>don’t let that dissuade you!</em></p><p>I’m not trying to discourage you from starting with local LLMs, but I do want you to know what to expect going in, so you don’t get frustrated or give up and write off running models locally altogether.</p><p><em>And</em>, this shouldn’t be taken as a slight against the thriving open source AI community which is doing mind-blowing things with far fewer resources than their well-funded commercial counterparts. In fact, the capabilities of the open source models are incredible and I’m a huge supporter of the open source AI community. I think it’s a critical counter-balance to a very possible tech dysotopia where the wrong people end up controlling AI and advanced robotics.</p><h2 id="what-running-models-locally-is-like-vs-the-frontier-models">What running models locally is like vs the frontier models</h2><p>But, simply for expectation setting, it’s important for people to know running Qwen3 30b locally isn’t going to be the same as chatting with GPT 5.2 or Opus 4.5. It’s going to be slower. It’s going to be less intelligent. It’s going to make more mistakes. You won’t be able to instantly flip into Deep Research mode and have it compile a 15 page report with 100 citations after reviewing 450 links.</p><p>Open source models like Qwen3 30b and GPT-OSS 20b and 120b are <em>insanely</em> capable <em>for their size</em>, marvelous feats of engineering, and it’s mind-bending that we have models as intelligent and capable as these running on our laptops, tablets, desktops, and even our phones, but these models aren’t trillions of parameters and consumer GPUs aren’t comparable to data centers, and that becomes apparent pretty fast after you get Ollama or LMstudio installed, pull your first model, and send your first few messages.</p><p>Those first few messages are incredible, don’t get me wrong. I know I felt a sense of awe, personally. This wasn’t something I thought I’d ever be able to do in my lifetime. Technology felt magical again — like logging onto the internet in 1995.</p><p>Then, my fan started roaring, my other apps started running slow, and I realized the responses I was getting weren’t what I was used to getting from the web-based paid chatbot solutions like ChatGPT, Gemini, or Claude.</p><p>When I asked for ideas, they weren’t as creative or considered, when I asked for writing, the quality was subpar. It didn’t have tools or integrations, it couldn’t make and edit files, it couldn’t search through our past discussions, so I needed to explain every bit of context it needed to do a good job. It’s context window was smaller so as the conversation continued, the quality of responses dropped off further.</p><p>It is <em>really</em> cool to run an AI locally on your computer, but it’s even more cool when it gives the right answer, has a great voice mode, is nearly instant, and the phone app is awesome. Also nice if I can connect to it Google Drive, Gmail, etc. That is to say, even if the hardware constraint weren’t there, the layers of invisible backend that facilitate the polished, robust chat experience we’ve come to expect, simply don’t exist for local, open source options… yet.</p><p>But just because an open source ~ 30b parameter model can’t compete head-to-head against models far, far above its weight class, that does not mean an equal or better user experience is unachievable with open source models running on consumer hardware. It just means it hasn’t been created yet and that’s <a href="https://heyally.ai">what we’re doing with Ally</a>.</p><h2 id="how-to-run-an-llm-locally">How to run an LLM locally</h2><p>Getting set up to run an LLM on your computer is easier than ever, but still requires some set up and command line usage. If you’d prefer to avoid using Terminal commands, you may want to skip Ollama and instead <a href="https://heyally.ai">use Ally</a>, which is the easest, simplest way to get started with <a href="https://heyally.ai/features/local-llm">local AI on a Mac</a>.</p><p>Ollama is another excellent choice, though it does require some command line.</p><h3 id="run-qwen3-8b-locally-with-ollama-step-by-step">Run Qwen3 8B locally with Ollama (step-by-step)</h3><ol><li>Install Ollama •	macOS: Download + install Ollama (requires macOS 14 Sonoma+). ￼ •	Windows: Download + run the installer (requires Windows 10+). ￼ •	Linux (recommended):</li></ol><pre><code>curl -fsSL https://ollama.com/install.sh | sh
</code></pre><ol start="2"><li>Start Ollama (Linux) + sanity check</li></ol><p>On Linux, start the server:</p><pre><code>ollama serve
</code></pre><p>Then confirm it’s installed:</p><pre><code>ollama -v
</code></pre><p>(On macOS/Windows, installing the app typically starts the Ollama service; the CLI should work once it’s running.) ￼</p><ol start="3"><li>Download (pull) Qwen3 8B</li></ol><pre><code>ollama pull qwen3:8b
</code></pre><ol start="4"><li>Run it</li></ol><pre><code>ollama run qwen3:8b
</code></pre><ol start="5"><li>Send a message using the local HTTP API</li></ol><pre><code>curl http://localhost:11434/api/chat -d &#39;{
  &quot;model&quot;: &quot;qwen3:8b&quot;,
  &quot;messages&quot;: [{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;Hello!&quot;}],
  &quot;stream&quot;: false
}&#39;
</code></pre><h2 id="how-to-run-an-llm-locally-with-ally">How to run an LLM locally with Ally</h2><p>Ally is a newer solution for running LLMs on device. It is an Apple-only solution, but if you have a Mac, there’s really no easier way to run inference on your Macbook or desktop.</p><ol><li><a href="macappstore://itunes.apple.com/app/id6752736179">Download Ally for macOS</a> from the app store.</li><li>Ally will automatically download Qwen3 8b the first time you open it.</li><li>Create a new node by clicking <code>+</code>, hover near the bottom of the window to display the chat input field, and send a message.</li></ol><p>That’s really it. No Docker or command line needed. You can switch to a different model in <code>Settings &gt; AI</code>.</p><p>Ally also gives you the advantage of a collaborative <a href="https://heyally.ai/features/markdown-editor">Markdown editor</a> with shared context, flexible, drag and drop file and chat organization, and more.</p></div><footer class="content__footer"><div class="entry-wrapper"><p class="content__updated">This article was updated on January 10, 2026</p><div class="content__actions"><ul class="content__tag"><li><a href="https://heyally.ai/blog/tags/ai/">ai</a></li><li><a href="https://heyally.ai/blog/tags/llms/">llms</a></li></ul></div></div></footer></article><div class="content__related related"><div class="wrapper"><h2 class="h4 related__title">You should also read:</h2><article class="feed__item feed__item--centered"><div class="feed__content"><header><h3 class="feed__title"><a href="https://heyally.ai/blog/tools-to-connect-to-many-llms-at-once/">Tools to connect to many LLMs at once</a></h3></header><p>To access the latest frontier AI models from Anthropic, OpenAI, Google, Perplexity, and Deepseek, you could pay $100 or more in monthly subscriptions. A cost-saving alternative is to use an AI chat interface — sometimes referred to as a “chatbot UI” or “LLM frontend” (there doesn’t appear to be an agreed upon naming convention). These apps allow you to connect to LLMs via their APIs. This way, you pay only for your actual usage. This approach&hellip;</p><a href="https://heyally.ai/blog/tools-to-connect-to-many-llms-at-once/" class="readmore feed__readmore">Continue reading...</a></div></article></div></div></main><footer class="footer footer--glued"><div class="wrapper"><div class="footer__copyright"><p><a href="https://heyally.ai">Back to Ally AI main site</a></p></div><button id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg width="20" height="20"><use xlink:href="https://heyally.ai/blog/assets/svg/svg-map.svg#toparrow"/></svg></button></div></footer><script defer="defer" src="https://heyally.ai/blog/assets/js/scripts.min.js?v=ffcbea6c02c8178d10092962b235a5b0"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script><script src="https://heyally.ai/blog/media/plugins/pagePrefetching/quicklink.umd.js"></script><script>window.addEventListener('load', () => {
					quicklink.listen({
						prerender: true,
						el: document.querySelector('body'),
						delay: 0,
						limit: Infinity,
						throttle: Infinity,
						timeout: 2000
					});
				});</script></body></html>